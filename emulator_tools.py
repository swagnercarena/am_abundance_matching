import george
from george.kernels import ExpSquaredKernel
import numpy as np
import scipy.optimize as op
from tqdm import tqdm

def generate_lhc(like_class,n_points,param_mins,param_maxs,n_wp_samps,
	random_seed=0, lhc_divisions=0, lhc_div_index=0):
	"""
	Generate a training dictionary based on a latin hypercube. This dictioanry
	will take into account variance in the am process for fixed parameters.

	Parameters:
		like_class: A galaxy_statistics.AMLikelihood object that has been
			initialized with the halo catalog.
		n_points: The number of points to include in our lhc.
		param_mins: A list of the minimum value for each param to use
			in the LHC
		param_maxs: A list of the maximum value for each param to use
			in the LHC
		n_wp_samps: The number of samples to use for each wprp to calculate
			the mean and the variance.
		random_seed: A random seed to initialize numpy with
		lhc_divisions: If you're running multiple generate_lhc calls in
			parallel, this value will tell the code how many peices the lhc
			should be divided into
		lhc_div_index: If running multiple calls, this value will tell the 
			which of the divisions to use

	Returns:
		A dicitonary object for each parameter including the mean and variance
			for each wprp.
	"""
	np.random.seed(random_seed)
	# Build the numpy array that will be the latin hypercube
	lhc = []
	for i in range(len(param_mins)):
		# Linearly space the parameters between the min and the max and
		# then convert to log space.
		lin_samp = np.log(np.linspace(param_mins[i],param_maxs[i],
			num=n_points))
		np.random.shuffle(lin_samp)
		lhc.append(lin_samp)
	lhc = np.stack(lhc).T

	if lhc_divisions > 0:
		n_point_div = n_points//lhc_divisions
		lhc = lhc[lhc_div_index*n_point_div:(lhc_div_index+1)*n_point_div]
	
	# Array to store samples. 2 is for the number wprp statistics.
	wp_samps = np.zeros((n_wp_samps,len(like_class.mag_cuts),
		len(like_class.r_p_data)))
	# Array to store the mean and std for each parameter
	wp_train_dict = {}
	for params in lhc:
		print('Working on scatter %.4f and mu_cut %.4f'%(np.exp(params[0]),
			np.exp(params[1])))
		# Sample the wprp statistics multiple times and save the samples
		for samp_i in tqdm(range(n_wp_samps)):
			wp_output = like_class.compute_wprp(params,verbose=False)
			for mag_i in range(len(wp_output)):
				wp_samps[samp_i,mag_i] = wp_output[mag_i]
		# Store the mean and variance in the training dictionary.
		wp_train_dict[tuple(params)] = {'mag_cuts': like_class.mag_cuts,
								 'means': np.mean(wp_samps,axis=0),
								'std': np.std(wp_samps,axis=0)}
	
	return wp_train_dict


def transform_in_out(wp_train_dict,rbins):
	"""
	Transform input and output space such that the inputs are abundance matching
	parameters and a radial bin, and the output is the value of wprp for that 
	combination of parameters and bin.

	Parameters:
		wp_train_dict: A dictionary generated by the generate_lhc class.
		rbins: The median value of the radial bins

	Returns:
		(am_param_train,wprp_train, wprp_err) - with the radial bins as one of 
			the input parameters.
	"""
	am_param_train = []
	wprp_train_arr = []
	wprp_err_arr = []
	for params in wp_train_dict:
		am_param_train.append(list(params))
		wprp_train_arr.append(wp_train_dict[params]['means'])
		wprp_err_arr.append(wp_train_dict[params]['std'])

	am_param_train = np.array(am_param_train)
	wprp_train_arr = np.array(wprp_train_arr)
	wprp_err_arr = np.array(wprp_err_arr)

	# Convert to a list of wprp for each magnitude cut bin.
	wprp_train = []
	wprp_err = []
	for mag_i in range(wprp_train_arr.shape[1]):
		wprp_train.append(wprp_train_arr[:,mag_i,:].flatten())
		wprp_err.append(wprp_err_arr[:,mag_i,:].flatten())

	# Initialize the shape of the new training inputs
	am_param_train_rbins = np.zeros((am_param_train.shape[0]*len(rbins),
		am_param_train.shape[1]+1))
	for ampi in range(len(am_param_train)):
		for rbinsi in range(len(rbins)):
			am_param_train_rbins[ampi*len(rbins)+rbinsi] = np.concatenate(
				[am_param_train[ampi],rbins[rbinsi]],axis=0)

	# Return all the objects we need for training.
	return am_param_train_rbins,wprp_train,wprp_err


def initialize_emulator(am_param_train,wprp_train,wprp_err):
	"""
	Given a set of abundance matching parameters and projectedtwo point 
	correlation functions to use for training, build an emulator.

	Parameters:
		am_param_train: A numpy array containing the abundance matching 
			parameters for each training point. This should have dimensions
			(n_training points x nun_params)
		wprp_train: The projected two point correlation function for each 
			set of abundance matching parameters

	Returns:
		An emulator initialized to the training points provided
	"""
	# Get the number of parameters for your abundance matching model
	n_am_params = am_param_train.shape[1]
	# Randomly initialzie our emulator parameters. The exact number of
	# parameters required depends on the kernel.
	em_vec = np.random.rand(n_am_params+2)

	sf = em_vec[0]
	sx = em_vec[-1]
	# This kernel was suggested by sean. Will likely have to experiment with
	# different kernel variaties
	kernel = sf * ExpSquaredKernel(em_vec[1:n_am_params+1], 
		ndim=n_am_params) + sx
	emulator = george.GP(kernel, mean=np.mean(wprp_train))
	emulator.compute(am_param_train,yerr=wprp_err)
	return emulator

def optimize_emulator(emulator,am_param_train,wprp_train,wprp_err):
	"""
	Find the local minimum of the emulator hyperparameters.

	Parameters:
		emulator: The emulator object (from George). Must have been initialized
		am_param_train: The am parameters to use to train the hyperparameter
			values
		wprp_train: The projected two point correlation functions for each of
			the abundance matching configurations

	Returns:
		None. The emulator object will be updated with the optimal
			hyperparameter values.
	"""
	def nll(vector):
		# Update the kernel params and calculate nll
		emulator.kernel.set_parameter_vector(vector)
		emulator.compute(am_param_train,yerr=wprp_err)
		ll = emulator.lnlikelihood(wprp_train, quiet=True)
		# Deal with scipy not liking infinities as per george example
		return -ll if np.isfinite(ll) else 1e25

	def grad_nll(vector):
		# Again update the kernel and calculate the grad of nll with respect
		# to the vector.
		emulator.kernel.set_parameter_vector(vector)
		emulator.compute(am_param_train,yerr=wprp_err)
		return -emulator.grad_lnlikelihood(wprp_train, quiet=True)

	# To start, run the standard scipy optimizer on our emulator
	vec0 = emulator.kernel.parameter_vector
	results = op.minimize(nll,vec0,jac=grad_nll, method="L-BFGS-B")

	# Update our kernel with the final step
	emulator.kernel.set_parameter_vector(results.x)


